import json
import pyshark
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import binascii
import re
from datetime import datetime
import os
from scipy import stats
import base64

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# é…ç½®ä¿¡æ¯
name_package_dict = {
    "å¥¥ç‰¹æ›¼": "(tcp.stream eq 8 and http2) && (ip.src == 198.18.0.7)",
    "å‡é¢éª‘å£«": "(tcp.stream eq 33 and http2) && (ip.src == 198.18.0.21)",
    "å°¼äºšåŠ æ‹‰ç€‘å¸ƒ": "(tcp.stream eq 70 and http2) && (ip.src == 198.18.0.14)",
    "å­™ç­–": "(tcp.stream eq 12 and http2) && (ip.src == 198.18.0.14)",
    "äº”å¤§æ¹–": "(tcp.stream eq 37 and http2) && (ip.src == 198.18.0.14)",
}

tshark_path = "D:\\else\\wireshark\\tshark.exe"

# åœ¨tshark_pathé…ç½®åæ·»åŠ ä»¥ä¸‹å‡½æ•°
def safe_int_convert(value, default=0):
    """å®‰å…¨åœ°å°†å„ç§ç±»å‹çš„å€¼è½¬æ¢ä¸ºæ•´æ•°"""
    if value is None:
        return default
    if isinstance(value, int):
        return value
    if isinstance(value, str):
        # å¤„ç†åå…­è¿›åˆ¶å­—ç¬¦ä¸²
        if value.startswith('0x'):
            try:
                return int(value, 16)
            except ValueError:
                return default
        # å¤„ç†æ™®é€šå­—ç¬¦ä¸²
        try:
            return int(value)
        except ValueError:
            return default
    try:
        return int(value)
    except (ValueError, TypeError):
        return default

def calculate_entropy(data):
    """è®¡ç®—æ•°æ®çš„ç†µå€¼"""
    if not data:
        return 0
    
    # è½¬æ¢ä¸ºå­—èŠ‚
    if isinstance(data, str):
        try:
            # å°è¯•è§£æåå…­è¿›åˆ¶
            if data.startswith('0x'):
                data = data[2:]
            data = bytes.fromhex(data.replace(':', '').replace(' ', ''))
        except:
            data = data.encode('utf-8')
    
    # è®¡ç®—å­—èŠ‚é¢‘ç‡
    byte_counts = Counter(data)
    total_bytes = len(data)
    
    # è®¡ç®—ç†µ
    entropy = 0
    for count in byte_counts.values():
        probability = count / total_bytes
        if probability > 0:
            entropy -= probability * np.log2(probability)
    
    return entropy

def analyze_padding(data):
    """åˆ†ææ•°æ®çš„å¡«å……æƒ…å†µ"""
    if not data:
        return {'padding_detected': False, 'padding_bytes': 0, 'padding_pattern': None}
    
    # è½¬æ¢ä¸ºå­—èŠ‚
    if isinstance(data, str):
        try:
            if data.startswith('0x'):
                data = data[2:]
            data = bytes.fromhex(data.replace(':', '').replace(' ', ''))
        except:
            data = data.encode('utf-8')
    
    # æ£€æŸ¥å¸¸è§çš„å¡«å……æ¨¡å¼
    padding_info = {
        'padding_detected': False,
        'padding_bytes': 0,
        'padding_pattern': None,
        'padding_type': None
    }
    
    if len(data) == 0:
        return padding_info
    
    # PKCS#7 å¡«å……æ£€æµ‹
    last_byte = data[-1]
    if last_byte <= 16:  # PKCS#7 æœ€å¤§å¡«å……é•¿åº¦
        potential_padding = data[-last_byte:]
        if all(b == last_byte for b in potential_padding):
            padding_info['padding_detected'] = True
            padding_info['padding_bytes'] = last_byte
            padding_info['padding_pattern'] = f"0x{last_byte:02x}"
            padding_info['padding_type'] = 'PKCS#7'
    
    # é›¶å¡«å……æ£€æµ‹
    trailing_zeros = 0
    for i in range(len(data) - 1, -1, -1):
        if data[i] == 0:
            trailing_zeros += 1
        else:
            break
    
    if trailing_zeros > 0 and not padding_info['padding_detected']:
        padding_info['padding_detected'] = True
        padding_info['padding_bytes'] = trailing_zeros
        padding_info['padding_pattern'] = "0x00"
        padding_info['padding_type'] = 'Zero Padding'
    
    return padding_info

def extract_text_content(data):
    """æå–æ•°æ®ä¸­çš„æ–‡æœ¬å†…å®¹"""
    text_content = {
        'readable_text': '',
        'text_ratio': 0,
        'contains_html': False,
        'contains_json': False,
        'contains_image_data': False
    }
    
    if not data:
        return text_content
    
    # è½¬æ¢ä¸ºå­—èŠ‚
    if isinstance(data, str):
        try:
            if data.startswith('0x'):
                data = data[2:]
            data = bytes.fromhex(data.replace(':', '').replace(' ', ''))
        except:
            data = data.encode('utf-8')
    
    # å°è¯•è§£ç ä¸ºæ–‡æœ¬
    try:
        text = data.decode('utf-8', errors='ignore')
        # è®¡ç®—å¯è¯»å­—ç¬¦æ¯”ä¾‹
        printable_chars = sum(1 for c in text if c.isprintable())
        text_content['text_ratio'] = printable_chars / len(text) if text else 0
        
        # æå–å¯è¯»æ–‡æœ¬
        readable_text = ''.join(c for c in text if c.isprintable())
        text_content['readable_text'] = readable_text[:500]  # é™åˆ¶é•¿åº¦
        
        # æ£€æµ‹å†…å®¹ç±»å‹
        text_lower = readable_text.lower()
        text_content['contains_html'] = '<html' in text_lower or '<!doctype' in text_lower
        text_content['contains_json'] = ('{' in text and '}' in text) or ('[' in text and ']' in text)
        
        # æ£€æµ‹å›¾ç‰‡æ•°æ®ç‰¹å¾
        image_signatures = [b'\xff\xd8\xff', b'\x89PNG', b'GIF8', b'RIFF']
        text_content['contains_image_data'] = any(sig in data for sig in image_signatures)
        
    except Exception as e:
        pass
    
    return text_content

def analyze_fragmentation(packets):
    """åˆ†ææ•°æ®åˆ†ç‰‡æƒ…å†µ"""
    fragmentation_info = {
        'total_fragments': len(packets),
        'fragment_sizes': [],
        'size_variance': 0,
        'reassembly_order': [],
        'gaps_detected': False
    }
    
    for packet in packets:
        if hasattr(packet, 'tcp'):
            seq_num = safe_int_convert(packet.tcp.seq)
            payload_len = safe_int_convert(getattr(packet.tcp, 'len', 0))
            fragmentation_info['fragment_sizes'].append(payload_len)
            fragmentation_info['reassembly_order'].append(seq_num)
    
    if fragmentation_info['fragment_sizes']:
        fragmentation_info['size_variance'] = np.var(fragmentation_info['fragment_sizes'])
    
    # æ£€æµ‹åºåˆ—å·é—´éš™
    if len(fragmentation_info['reassembly_order']) > 1:
        sorted_seqs = sorted(fragmentation_info['reassembly_order'])
        for i in range(1, len(sorted_seqs)):
            if sorted_seqs[i] - sorted_seqs[i-1] > max(fragmentation_info['fragment_sizes']):
                fragmentation_info['gaps_detected'] = True
                break
    
    return fragmentation_info

def deep_packet_analysis(cap_file, target_name):
    """æ·±åº¦æ•°æ®åŒ…åˆ†æ"""
    # è·å–å¯¹åº”çš„tsharkè¿‡æ»¤å™¨
    display_filter = name_package_dict.get(target_name, "")
    print(f"ä½¿ç”¨è¿‡æ»¤å™¨: {display_filter}")
    
    # ä½¿ç”¨tsharkè·¯å¾„å’Œè¿‡æ»¤å™¨
    cap = pyshark.FileCapture(cap_file, tshark_path=tshark_path, display_filter=display_filter)
    analysis_results = {
        'timestamp': datetime.now().isoformat(),
        'total_packets': 0,
        'detailed_analysis': [],
        'encryption_analysis': {
            'entropy_comparison': [],
            'length_expansion': [],
            'padding_analysis': [],
            'fragmentation_patterns': []
        },
        'content_analysis': {
            'text_extraction': [],
            'data_types': Counter(),
            'compression_indicators': []
        }
    }
    
    packets_by_stream = {}
    
    for packet in cap:
        analysis_results['total_packets'] += 1
        
        packet_analysis = {
            'packet_number': safe_int_convert(packet.number),
            'timestamp': str(packet.sniff_time),
            'layers': {}
        }
        
        # åˆ†æå„å±‚æ•°æ®
        for layer in packet.layers:
            layer_name = layer.layer_name
            layer_analysis = {'raw_data': None, 'decoded_data': None}
            
            if layer_name == 'tcp':
                tcp_data = {
                    'sequence_number': safe_int_convert(getattr(layer, 'seq', 0)),
                    'payload_length': safe_int_convert(getattr(layer, 'len', 0)),
                    'flags': str(layer.flags_str) if hasattr(layer, 'flags_str') else '',
                    'window_size': safe_int_convert(getattr(layer, 'window_size_value', 0))
                }
                
                # æå–TCPè½½è·
                if hasattr(layer, 'payload'):
                    tcp_data['payload_hex'] = str(layer.payload)
                    tcp_data['payload_entropy'] = calculate_entropy(str(layer.payload))
                
                layer_analysis['decoded_data'] = tcp_data
                
            elif layer_name == 'tls':
                tls_data = {
                    'record_type': safe_int_convert(getattr(layer, 'record_content_type', 0)),
                    'version': str(layer.record_version) if hasattr(layer, 'record_version') else '',
                    'encrypted_length': safe_int_convert(getattr(layer, 'record_length', 0))
                }
                
                # æå–åŠ å¯†æ•°æ®
                if hasattr(layer, 'app_data'):
                    encrypted_data = str(layer.app_data)
                    tls_data['encrypted_data_hex'] = encrypted_data
                    tls_data['encrypted_entropy'] = calculate_entropy(encrypted_data)
                    tls_data['padding_analysis'] = analyze_padding(encrypted_data)
                
                layer_analysis['decoded_data'] = tls_data
                
            elif layer_name == 'http2':
                http2_data = {
                    'stream_id': safe_int_convert(getattr(layer, 'streamid', 0)),
                    'frame_type': safe_int_convert(getattr(layer, 'type', 0)),
                    'frame_length': safe_int_convert(getattr(layer, 'length', 0)),
                    'flags': safe_int_convert(getattr(layer, 'flags', 0))
                }
                
                # æå–HTTP2æ•°æ®
                if hasattr(layer, 'data'):
                    plaintext_data = str(layer.data)
                    http2_data['plaintext_hex'] = plaintext_data
                    http2_data['plaintext_entropy'] = calculate_entropy(plaintext_data)
                    http2_data['text_content'] = extract_text_content(plaintext_data)
                    http2_data['padding_analysis'] = analyze_padding(plaintext_data)
                
                # æå–å¤´éƒ¨ä¿¡æ¯
                headers = {}
                for field_name in layer.field_names:
                    if field_name.startswith('headers_'):
                        header_name = field_name.replace('headers_', '')
                        headers[header_name] = str(getattr(layer, field_name))
                
                http2_data['headers'] = headers
                layer_analysis['decoded_data'] = http2_data
            
            packet_analysis['layers'][layer_name] = layer_analysis
        
        # è¿›è¡Œå¯†æ–‡æ˜æ–‡å¯¹æ¯”åˆ†æ
        if 'tls' in packet_analysis['layers'] and 'http2' in packet_analysis['layers']:
            tls_layer = packet_analysis['layers']['tls']['decoded_data']
            http2_layer = packet_analysis['layers']['http2']['decoded_data']
            
            comparison = {
                'packet_number': packet_analysis['packet_number'],
                'plaintext_length': http2_layer.get('frame_length', 0),
                'ciphertext_length': tls_layer.get('encrypted_length', 0),
                'length_expansion_ratio': 0,
                'entropy_difference': 0,
                'compression_detected': False
            }
            
            if comparison['plaintext_length'] > 0:
                comparison['length_expansion_ratio'] = comparison['ciphertext_length'] / comparison['plaintext_length']
            
            # ç†µå€¼å¯¹æ¯”
            plaintext_entropy = http2_layer.get('plaintext_entropy', 0)
            ciphertext_entropy = tls_layer.get('encrypted_entropy', 0)
            comparison['entropy_difference'] = ciphertext_entropy - plaintext_entropy
            
            # å‹ç¼©æ£€æµ‹
            if comparison['length_expansion_ratio'] < 1.0:
                comparison['compression_detected'] = True
            
            analysis_results['encryption_analysis']['entropy_comparison'].append(comparison)
        
        analysis_results['detailed_analysis'].append(packet_analysis)
        
        # æŒ‰æµåˆ†ç»„
        if 'http2' in packet_analysis['layers']:
            stream_id = packet_analysis['layers']['http2']['decoded_data'].get('stream_id', 0)
            if stream_id not in packets_by_stream:
                packets_by_stream[stream_id] = []
            packets_by_stream[stream_id].append(packet)
    
    # åˆ†æåˆ†ç‰‡æ¨¡å¼
    for stream_id, stream_packets in packets_by_stream.items():
        if len(stream_packets) > 1:
            frag_analysis = analyze_fragmentation(stream_packets)
            frag_analysis['stream_id'] = stream_id
            analysis_results['encryption_analysis']['fragmentation_patterns'].append(frag_analysis)
    
    cap.close()
    return analysis_results

def create_deep_analysis_charts(name, analysis_data, output_dir):
    """åˆ›å»ºä¼˜åŒ–çš„æ·±åº¦åˆ†æå›¾è¡¨"""
    fig = plt.figure(figsize=(16, 12))
    
    # 1. ç†µå€¼å¯¹æ¯”åˆ†æ - æ ¸å¿ƒåŠ å¯†åˆ†æ
    ax1 = plt.subplot(2, 3, 1)
    entropy_data = analysis_data['encryption_analysis']['entropy_comparison']
    if entropy_data:
        packets = [d['packet_number'] for d in entropy_data]
        plaintext_entropies = []
        ciphertext_entropies = []
        
        # åœ¨create_deep_analysis_chartså‡½æ•°ä¸­
        for packet_data in analysis_data['detailed_analysis']:
            if 'tcp' in packet_data['layers'] and 'tls' in packet_data['layers']:
                tcp_entropy = packet_data['layers']['tcp']['decoded_data'].get('payload_entropy', 0)
                tls_entropy = packet_data['layers']['tls']['decoded_data'].get('encrypted_entropy', 0)
                plaintext_entropies.append(tcp_entropy)  # ä½¿ç”¨TCPè½½è·ç†µå€¼
                ciphertext_entropies.append(tls_entropy)
        
        if plaintext_entropies and ciphertext_entropies:
            x = np.arange(len(packets))
            width = 0.35
            ax1.bar(x - width/2, plaintext_entropies, width, label='æ˜æ–‡ç†µå€¼', alpha=0.8, color='lightblue')
            ax1.bar(x + width/2, ciphertext_entropies, width, label='å¯†æ–‡ç†µå€¼', alpha=0.8, color='lightcoral')
            ax1.set_title('æ˜æ–‡vså¯†æ–‡ç†µå€¼å¯¹æ¯”', fontsize=12, fontweight='bold')
            ax1.set_xlabel('æ•°æ®åŒ…')
            ax1.set_ylabel('ç†µå€¼')
            ax1.legend()
            ax1.set_xticks(x)
            ax1.set_xticklabels([f'#{p}' for p in packets], rotation=45)
            ax1.grid(True, alpha=0.3)
    
    # 2. é•¿åº¦æ‰©å¼ åˆ†æ - åŠ å¯†å¼€é”€åˆ†æ
    ax2 = plt.subplot(2, 3, 2)
    if entropy_data:
        expansion_ratios = [d['length_expansion_ratio'] for d in entropy_data if d['length_expansion_ratio'] > 0]
        packet_nums = [d['packet_number'] for d in entropy_data if d['length_expansion_ratio'] > 0]
        
        if expansion_ratios:
            colors = ['red' if r > 1.2 else 'orange' if r > 1.0 else 'green' for r in expansion_ratios]
            bars = ax2.bar(range(len(expansion_ratios)), expansion_ratios, color=colors, alpha=0.7)
            ax2.set_title('å¯†æ–‡é•¿åº¦æ‰©å¼ ç‡', fontsize=12, fontweight='bold')
            ax2.set_xlabel('æ•°æ®åŒ…')
            ax2.set_ylabel('æ‰©å¼ ç‡')
            ax2.axhline(y=1.0, color='black', linestyle='--', alpha=0.5, label='åŸºå‡†çº¿')
            ax2.set_xticks(range(len(packet_nums)))
            ax2.set_xticklabels([f'#{p}' for p in packet_nums], rotation=45)
            ax2.legend()
            ax2.grid(True, alpha=0.3)
    
    # 3. å¡«å……ç±»å‹åˆ†æ
    ax3 = plt.subplot(2, 3, 3)
    padding_types = Counter()
    
    for packet_data in analysis_data['detailed_analysis']:
        for layer_name, layer_data in packet_data['layers'].items():
            if 'decoded_data' in layer_data and layer_data['decoded_data']:
                padding_info = layer_data['decoded_data'].get('padding_analysis')
                if padding_info:
                    if padding_info.get('padding_detected'):
                        padding_type = padding_info.get('padding_type', 'Unknown')
                        padding_types[padding_type] += 1
                    else:
                        padding_types['No Padding'] += 1
    
    if padding_types:
        labels = list(padding_types.keys())
        sizes = list(padding_types.values())
        colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']
        ax3.pie(sizes, labels=labels, colors=colors[:len(labels)], autopct='%1.1f%%', startangle=90)
        ax3.set_title('å¡«å……ç±»å‹åˆ†å¸ƒ', fontsize=12, fontweight='bold')
    
    # 4. æ•°æ®åˆ†ç‰‡å¤§å°åˆ†å¸ƒ
    ax4 = plt.subplot(2, 3, 4)
    fragment_sizes = []
    for frag_pattern in analysis_data['encryption_analysis']['fragmentation_patterns']:
        if frag_pattern.get('fragment_sizes'):
            fragment_sizes.extend(frag_pattern['fragment_sizes'])
    
    if fragment_sizes:
        ax4.hist(fragment_sizes, bins=min(20, len(set(fragment_sizes))), alpha=0.7, color='skyblue', edgecolor='black')
        ax4.set_title('æ•°æ®åˆ†ç‰‡å¤§å°åˆ†å¸ƒ', fontsize=12, fontweight='bold')
        ax4.set_xlabel('åˆ†ç‰‡å¤§å° (å­—èŠ‚)')
        ax4.set_ylabel('é¢‘æ¬¡')
        ax4.grid(True, alpha=0.3)
    
    # 5. æ”¹è¿›çš„æ•°æ®æµæ—¶é—´çº¿
    ax5 = plt.subplot(2, 3, 5)
    timestamps = []
    data_sizes = []
    packet_numbers = []
    
    for packet_data in analysis_data['detailed_analysis']:
        try:
            # ä½¿ç”¨æ•°æ®åŒ…åºå·ä½œä¸ºæ—¶é—´è½´ï¼Œæ›´ç¨³å®š
            packet_numbers.append(packet_data['packet_number'])
            
            total_size = 0
            for layer_data in packet_data['layers'].values():
                if 'decoded_data' in layer_data and layer_data['decoded_data']:
                    # å°è¯•å¤šç§å¤§å°å­—æ®µ
                    size_fields = ['frame_length', 'length', 'data_length', 'payload_length']
                    for field in size_fields:
                        if field in layer_data['decoded_data']:
                            size_val = layer_data['decoded_data'][field]
                            if isinstance(size_val, (int, float)) and size_val > 0:
                                total_size += size_val
                                break
            
            if total_size > 0:
                data_sizes.append(total_size)
            else:
                data_sizes.append(0)
        except Exception as e:
            packet_numbers.append(packet_data.get('packet_number', len(packet_numbers)))
            data_sizes.append(0)
    
    if packet_numbers and data_sizes and any(s > 0 for s in data_sizes):
        ax5.plot(packet_numbers, data_sizes, marker='o', linewidth=2, markersize=4, color='blue')
        ax5.set_title('æ•°æ®åŒ…å¤§å°å˜åŒ–è¶‹åŠ¿', fontsize=12, fontweight='bold')
        ax5.set_xlabel('æ•°æ®åŒ…åºå·')
        ax5.set_ylabel('æ•°æ®å¤§å° (å­—èŠ‚)')
        ax5.grid(True, alpha=0.3)
    
    # 6. å‹ç¼©/æ‰©å¼ æ•ˆæœåˆ†æ
    ax6 = plt.subplot(2, 3, 6)
    if entropy_data:
        compression_stats = {'å‹ç¼©': 0, 'æ‰©å¼ ': 0, 'æ— å˜åŒ–': 0}
        
        for comp_data in entropy_data:
            ratio = comp_data['length_expansion_ratio']
            if ratio < 0.95:
                compression_stats['å‹ç¼©'] += 1
            elif ratio > 1.05:
                compression_stats['æ‰©å¼ '] += 1
            else:
                compression_stats['æ— å˜åŒ–'] += 1
        
        if any(compression_stats.values()):
            labels = list(compression_stats.keys())
            sizes = list(compression_stats.values())
            colors = ['green', 'red', 'gray']
            wedges, texts, autotexts = ax6.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
            ax6.set_title('æ•°æ®å‹ç¼©/æ‰©å¼ åˆ†å¸ƒ', fontsize=12, fontweight='bold')
            
            # ç¾åŒ–é¥¼å›¾æ–‡å­—
            for autotext in autotexts:
                autotext.set_color('white')
                autotext.set_fontweight('bold')
    
    plt.tight_layout(pad=3.0)
    chart_file = os.path.join(output_dir, f'{name}_æ·±åº¦åˆ†æå›¾è¡¨.png')
    plt.savefig(chart_file, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    return chart_file

def generate_deep_analysis_report(name, analysis_data, output_dir):
    """ç”Ÿæˆæ·±åº¦åˆ†ææŠ¥å‘Š"""
    report = []
    
    # æŠ¥å‘Šæ ‡é¢˜
    report.append("="*80)
    report.append("ç½‘ç»œæ•°æ®åŒ…æ·±åº¦åˆ†ææŠ¥å‘Š")
    report.append("å¯†æ–‡ä¸æ˜æ–‡ç‰¹å¾å¯¹æ¯”åˆ†æ")
    report.append("="*80)
    report.append(f"åˆ†ææ—¶é—´: {analysis_data['timestamp']}")
    report.append(f"æ€»æ•°æ®åŒ…æ•°: {analysis_data['total_packets']}")
    report.append("")
    
    # åˆå§‹åŒ–å˜é‡
    avg_expansion = 0
    avg_entropy_diff = 0
    compression_count = 0
    
    # åŠ å¯†åˆ†ææ¦‚è§ˆ
    entropy_data = analysis_data['encryption_analysis']['entropy_comparison']
    report.append("ğŸ” åŠ å¯†ç‰¹å¾åˆ†ææ¦‚è§ˆ")
    report.append("-"*50)
    
    if entropy_data:
        avg_expansion = np.mean([d['length_expansion_ratio'] for d in entropy_data if d['length_expansion_ratio'] > 0])
        avg_entropy_diff = np.mean([d['entropy_difference'] for d in entropy_data])
        compression_count = sum(1 for d in entropy_data if d['compression_detected'])
        
        report.append(f"å¹³å‡é•¿åº¦æ‰©å¼ ç‡: {avg_expansion:.3f}")
        report.append(f"å¹³å‡ç†µå€¼å¢åŠ : {avg_entropy_diff:.3f}")
        report.append(f"æ£€æµ‹åˆ°å‹ç¼©çš„æ•°æ®åŒ…: {compression_count}/{len(entropy_data)}")
        report.append("")
    
    # è¯¦ç»†åˆ†æè¡¨æ ¼
    report.append("ğŸ“Š å¯†æ–‡æ˜æ–‡å¯¹æ¯”è¯¦ç»†åˆ†æ")
    report.append("-"*50)
    report.append(f"{'åŒ…å·':<8} {'æ˜æ–‡é•¿åº¦':<10} {'å¯†æ–‡é•¿åº¦':<10} {'æ‰©å¼ ç‡':<8} {'ç†µå€¼å·®':<8} {'å‹ç¼©':<6}")
    report.append("-"*60)
    
    for data in entropy_data:
        compression_mark = "æ˜¯" if data['compression_detected'] else "å¦"
        report.append(f"{data['packet_number']:<8} {data['plaintext_length']:<10} {data['ciphertext_length']:<10} "
                     f"{data['length_expansion_ratio']:<8.3f} {data['entropy_difference']:<8.3f} {compression_mark:<6}")
    
    # å¡«å……åˆ†æ
    report.append("\nğŸ”§ æ•°æ®å¡«å……åˆ†æ")
    report.append("-"*50)
    
    padding_stats = {'PKCS#7': 0, 'Zero Padding': 0, 'No Padding': 0}
    total_padding_bytes = 0
    
    for packet_data in analysis_data['detailed_analysis']:
        for layer_data in packet_data['layers'].values():
            if 'decoded_data' in layer_data and layer_data['decoded_data']:
                padding_info = layer_data['decoded_data'].get('padding_analysis')
                if padding_info:
                    if padding_info.get('padding_detected'):
                        padding_type = padding_info.get('padding_type', 'Unknown')
                        padding_stats[padding_type] = padding_stats.get(padding_type, 0) + 1
                        total_padding_bytes += padding_info.get('padding_bytes', 0)
                    else:
                        padding_stats['No Padding'] += 1
    
    for padding_type, count in padding_stats.items():
        report.append(f"{padding_type}: {count} æ¬¡")
    report.append(f"æ€»å¡«å……å­—èŠ‚æ•°: {total_padding_bytes}")
    
    # å†…å®¹ç±»å‹åˆ†æ
    report.append("\nğŸ“„ æ•°æ®å†…å®¹åˆ†æ")
    report.append("-"*50)
    
    content_analysis = []
    for packet_data in analysis_data['detailed_analysis']:
        if 'http2' in packet_data['layers']:
            text_content = packet_data['layers']['http2']['decoded_data'].get('text_content', {})
            if text_content.get('readable_text'):
                content_analysis.append({
                    'packet': packet_data['packet_number'],
                    'text_ratio': text_content.get('text_ratio', 0),
                    'content_type': 'HTML' if text_content.get('contains_html') else 
                                   'JSON' if text_content.get('contains_json') else
                                   'Image' if text_content.get('contains_image_data') else 'Text',
                    'sample_text': text_content.get('readable_text', '')[:100]
                })
    
    report.append(f"{'åŒ…å·':<8} {'å¯è¯»ç‡':<8} {'ç±»å‹':<8} {'å†…å®¹æ ·æœ¬':<50}")
    report.append("-"*80)
    for content in content_analysis[:10]:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
        report.append(f"{content['packet']:<8} {content['text_ratio']:<8.2f} {content['content_type']:<8} "
                     f"{content['sample_text']:<50}")
    
    # åˆ†ç‰‡åˆ†æ
    report.append("\nğŸ§© æ•°æ®åˆ†ç‰‡ä¸é‡ç»„åˆ†æ")
    report.append("-"*50)
    
    for frag_pattern in analysis_data['encryption_analysis']['fragmentation_patterns']:
        report.append(f"æµ ID {frag_pattern['stream_id']}:")
        report.append(f"  åˆ†ç‰‡æ•°é‡: {frag_pattern['total_fragments']}")
        report.append(f"  å¤§å°æ–¹å·®: {frag_pattern['size_variance']:.2f}")
        report.append(f"  æ£€æµ‹åˆ°é—´éš™: {'æ˜¯' if frag_pattern['gaps_detected'] else 'å¦'}")
        if frag_pattern['fragment_sizes']:
            avg_size = np.mean(frag_pattern['fragment_sizes'])
            report.append(f"  å¹³å‡åˆ†ç‰‡å¤§å°: {avg_size:.2f} å­—èŠ‚")
        report.append("")
    
    # å®‰å…¨æ€§è¯„ä¼°
    report.append("ğŸ›¡ï¸ å®‰å…¨æ€§è¯„ä¼°")
    report.append("-"*50)
    
    # ç†µå€¼è¯„ä¼°
    high_entropy_count = 0
    low_entropy_count = 0
    
    for packet_data in analysis_data['detailed_analysis']:
        if 'tls' in packet_data['layers']:
            entropy = packet_data['layers']['tls']['decoded_data'].get('encrypted_entropy', 0)
            if entropy > 7.5:  # é«˜ç†µå€¼é˜ˆå€¼
                high_entropy_count += 1
            elif entropy < 6.0:  # ä½ç†µå€¼é˜ˆå€¼
                low_entropy_count += 1
    
    report.append(f"é«˜ç†µå€¼å¯†æ–‡åŒ… (>7.5): {high_entropy_count}")
    report.append(f"ä½ç†µå€¼å¯†æ–‡åŒ… (<6.0): {low_entropy_count}")
    
    if high_entropy_count > low_entropy_count:
        report.append("âœ… åŠ å¯†è´¨é‡è‰¯å¥½ï¼Œç†µå€¼åˆ†å¸ƒæ­£å¸¸")
    else:
        report.append("âš ï¸  éƒ¨åˆ†å¯†æ–‡ç†µå€¼åä½ï¼Œå¯èƒ½å­˜åœ¨æ¨¡å¼")
    
    # å»ºè®®
    report.append("\nğŸ’¡ ä¼˜åŒ–å»ºè®®")
    report.append("-"*50)
    
    if avg_expansion > 1.2:
        report.append("â€¢ è€ƒè™‘ä¼˜åŒ–åŠ å¯†ç®—æ³•æˆ–å‡å°‘å¡«å……å¼€é”€")
    if compression_count > 0:
        report.append("â€¢ æ£€æµ‹åˆ°æ•°æ®å‹ç¼©ï¼Œå»ºè®®åˆ†æå‹ç¼©ç®—æ³•æ•ˆæœ")
    if total_padding_bytes > 100:
        report.append("â€¢ å¡«å……å­—èŠ‚è¾ƒå¤šï¼Œå¯è€ƒè™‘ä¼˜åŒ–å¡«å……ç­–ç•¥")
    
    report.append("\n" + "="*80)
    report.append("æ·±åº¦åˆ†ææŠ¥å‘Šå®Œæˆ")
    report.append("="*80)
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = os.path.join(output_dir, f'{name}_æ·±åº¦åˆ†ææŠ¥å‘Š.txt')
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report))
    
    # ä¿å­˜JSONæ•°æ®
    json_file = os.path.join(output_dir, f'{name}_æ·±åº¦åˆ†ææ•°æ®.json')
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(analysis_data, f, ensure_ascii=False, indent=2, default=str)
    
    return report_file, json_file

def main(target_name):
    """ä¸»å‡½æ•°"""
    # å¯é€‰æ‹©çš„åˆ†æç›®æ ‡
    available_targets = list(name_package_dict.keys())
    print("å¯ç”¨çš„æ·±åº¦åˆ†æç›®æ ‡:")
    for i, target in enumerate(available_targets, 1):
        print(f"{i}. {target}")
        
    print(f"\nå½“å‰æ·±åº¦åˆ†æç›®æ ‡: {target_name}")
    print(f"å¯¹åº”çš„è¿‡æ»¤å™¨: {name_package_dict[target_name]}")
    
    # æ„å»ºpcapæ–‡ä»¶è·¯å¾„
    current_dir = os.path.dirname(os.path.abspath(__file__))
    pcap_file = os.path.join(current_dir, f'{target_name}.pcapng')
    
    if not os.path.exists(pcap_file):
        print(f"æ–‡ä»¶ä¸å­˜åœ¨: {pcap_file}")
        return
    
    print("å¼€å§‹æ·±åº¦åˆ†æ...")
    
    try:
        # æ‰§è¡Œæ·±åº¦åˆ†æï¼Œä¼ å…¥target_nameå‚æ•°ä»¥ä½¿ç”¨å¯¹åº”çš„è¿‡æ»¤å™¨
        analysis_data = deep_packet_analysis(pcap_file, target_name)
        
        # ç”Ÿæˆå›¾è¡¨
        print("ç”Ÿæˆåˆ†æå›¾è¡¨...")
        chart_file = create_deep_analysis_charts(target_name, analysis_data, current_dir)
        print(f"å›¾è¡¨å·²ä¿å­˜: {os.path.basename(chart_file)}")
        
        # ç”ŸæˆæŠ¥å‘Š
        print("ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        report_file, json_file = generate_deep_analysis_report(target_name, analysis_data, current_dir)
        print(f"æŠ¥å‘Šå·²ä¿å­˜: {os.path.basename(report_file)}")
        print(f"æ•°æ®å·²ä¿å­˜: {os.path.basename(json_file)}")
        
        print("\næ·±åº¦åˆ†æå®Œæˆï¼")
        
    except Exception as e:
        print(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main('äº”å¤§æ¹–')
    main('å¥¥ç‰¹æ›¼')
    main('å‡é¢éª‘å£«')
    main('å°¼äºšåŠ æ‹‰ç€‘å¸ƒ')
    main('å­™ç­–')