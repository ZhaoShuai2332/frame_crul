import json
import pyshark
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import binascii
import re
from datetime import datetime
import os
from scipy import stats
import base64

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# é…ç½®ä¿¡æ¯
name_package_dict = {
    "å¥¥ç‰¹æ›¼": "(tcp.stream eq 8 and http2) && (ip.src == 198.18.0.7)",
    "å‡é¢éª‘å£«": "(tcp.stream eq 33 and http2) && (ip.src == 198.18.0.21)",
    "å°¼äºšåŠ æ‹‰ç€‘å¸ƒ": "(tcp.stream eq 70 and http2) && (ip.src == 198.18.0.14)",
    "å­™ç­–": "(tcp.stream eq 12 and http2) && (ip.src == 198.18.0.14)",
    "äº”å¤§æ¹–": "(tcp.stream eq 37 and http2) && (ip.src == 198.18.0.14)",
}

tshark_path = "D:\\else\\wireshark\\tshark.exe"

# åœ¨tshark_pathé…ç½®åæ·»åŠ ä»¥ä¸‹å‡½æ•°
def safe_int_convert(value, default=0):
    """å®‰å…¨åœ°å°†å„ç§ç±»å‹çš„å€¼è½¬æ¢ä¸ºæ•´æ•°"""
    if value is None:
        return default
    if isinstance(value, int):
        return value
    if isinstance(value, str):
        # å¤„ç†åå…­è¿›åˆ¶å­—ç¬¦ä¸²
        if value.startswith('0x'):
            try:
                return int(value, 16)
            except ValueError:
                return default
        # å¤„ç†æ™®é€šå­—ç¬¦ä¸²
        try:
            return int(value)
        except ValueError:
            return default
    try:
        return int(value)
    except (ValueError, TypeError):
        return default

def calculate_entropy(data):
    """è®¡ç®—æ•°æ®çš„ç†µå€¼"""
    if not data:
        return 0
    
    # è½¬æ¢ä¸ºå­—èŠ‚
    if isinstance(data, str):
        try:
            # å°è¯•è§£æåå…­è¿›åˆ¶
            if data.startswith('0x'):
                data = data[2:]
            data = bytes.fromhex(data.replace(':', '').replace(' ', ''))
        except:
            data = data.encode('utf-8')
    
    # è®¡ç®—å­—èŠ‚é¢‘ç‡
    byte_counts = Counter(data)
    total_bytes = len(data)
    
    # è®¡ç®—ç†µ
    entropy = 0
    for count in byte_counts.values():
        probability = count / total_bytes
        if probability > 0:
            entropy -= probability * np.log2(probability)
    
    return entropy

def analyze_advanced_padding(data):
    """å¢å¼ºçš„æ•°æ®å¡«å……åˆ†æ"""
    if not data:
        return {'padding_detected': False, 'padding_bytes': 0, 'padding_pattern': None}
    
    # è½¬æ¢ä¸ºå­—èŠ‚
    if isinstance(data, str):
        try:
            if data.startswith('0x'):
                data = data[2:]
            data = bytes.fromhex(data.replace(':', '').replace(' ', ''))
        except:
            data = data.encode('utf-8')
    
    padding_info = {
        'padding_detected': False,
        'padding_bytes': 0,
        'padding_pattern': None,
        'padding_type': None,
        'padding_content': None,
        'protocol_indicators': [],
        'entropy_analysis': {},
        'pattern_analysis': {},
        'block_alignment': {}
    }
    
    if len(data) == 0:
        return padding_info
    
    # 1. PKCS#7 å¡«å……æ£€æµ‹ï¼ˆå¢å¼ºç‰ˆï¼‰
    last_byte = data[-1]
    if 1 <= last_byte <= 32:  # æ‰©å±•æ£€æµ‹èŒƒå›´
        if len(data) >= last_byte:
            potential_padding = data[-last_byte:]
            if all(b == last_byte for b in potential_padding):
                padding_info.update({
                    'padding_detected': True,
                    'padding_bytes': last_byte,
                    'padding_pattern': f"0x{last_byte:02x}",
                    'padding_type': 'PKCS#7',
                    'padding_content': potential_padding.hex(':'),
                    'protocol_indicators': ['TLS', 'AES-CBC', 'SSL']
                })
                
                # åˆ†æå¡«å……å‰çš„æ•°æ®
                payload_data = data[:-last_byte]
                padding_info['entropy_analysis'] = {
                    'payload_entropy': calculate_entropy(payload_data),
                    'padding_entropy': calculate_entropy(potential_padding),
                    'total_entropy': calculate_entropy(data)
                }
    
    # 2. é›¶å¡«å……æ£€æµ‹ï¼ˆå¢å¼ºç‰ˆï¼‰
    if not padding_info['padding_detected']:
        trailing_zeros = 0
        for i in range(len(data) - 1, -1, -1):
            if data[i] == 0:
                trailing_zeros += 1
            else:
                break
        
        if trailing_zeros > 0:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ„ä¹‰çš„é›¶å¡«å……
            if trailing_zeros >= 4 or trailing_zeros == len(data) % 8:
                padding_info.update({
                    'padding_detected': True,
                    'padding_bytes': trailing_zeros,
                    'padding_pattern': "0x00",
                    'padding_type': 'Zero Padding',
                    'padding_content': '00:' * trailing_zeros,
                    'protocol_indicators': ['IPSec', 'Custom Protocol']
                })
    
    # 3. ANSI X9.23 å¡«å……æ£€æµ‹
    if not padding_info['padding_detected'] and len(data) >= 2:
        last_byte = data[-1]
        if 1 <= last_byte <= 16 and len(data) >= last_byte:
            potential_padding = data[-last_byte:]
            # ANSI X9.23: å‰é¢éƒ½æ˜¯0ï¼Œæœ€åä¸€ä¸ªå­—èŠ‚æ˜¯é•¿åº¦
            if all(b == 0 for b in potential_padding[:-1]) and potential_padding[-1] == last_byte:
                padding_info.update({
                    'padding_detected': True,
                    'padding_bytes': last_byte,
                    'padding_pattern': f"00...{last_byte:02x}",
                    'padding_type': 'ANSI X9.23',
                    'padding_content': potential_padding.hex(':'),
                    'protocol_indicators': ['Legacy SSL', 'Custom Crypto']
                })
    
    # 4. ISO 10126 å¡«å……æ£€æµ‹ï¼ˆéšæœºå¡«å……ï¼‰
    if not padding_info['padding_detected'] and len(data) >= 2:
        last_byte = data[-1]
        if 1 <= last_byte <= 16 and len(data) >= last_byte:
            potential_padding = data[-last_byte:]
            # ISO 10126: éšæœºå­—èŠ‚ + é•¿åº¦å­—èŠ‚
            if potential_padding[-1] == last_byte:
                # æ£€æŸ¥éšæœºæ€§ï¼ˆç†µå€¼åº”è¯¥è¾ƒé«˜ï¼‰
                random_part = potential_padding[:-1]
                if len(random_part) > 0:
                    random_entropy = calculate_entropy(random_part)
                    if random_entropy > 3.0:  # ç›¸å¯¹é«˜çš„ç†µå€¼è¡¨ç¤ºéšæœºæ€§
                        padding_info.update({
                            'padding_detected': True,
                            'padding_bytes': last_byte,
                            'padding_pattern': f"random+{last_byte:02x}",
                            'padding_type': 'ISO 10126',
                            'padding_content': potential_padding.hex(':'),
                            'protocol_indicators': ['Modern TLS', 'Advanced Crypto']
                        })
    
    # 5. å—å¯¹é½åˆ†æ
    common_block_sizes = [8, 16, 32, 64]
    for block_size in common_block_sizes:
        if len(data) % block_size == 0:
            padding_info['block_alignment'][f'{block_size}_byte'] = True
            if block_size == 16:
                padding_info['protocol_indicators'].extend(['AES', 'TLS 1.2+'])
            elif block_size == 8:
                padding_info['protocol_indicators'].extend(['3DES', 'Legacy SSL'])
    
    # 6. æ¨¡å¼åˆ†æ
    if len(data) >= 16:
        # æ£€æŸ¥é‡å¤æ¨¡å¼
        patterns = {}
        for i in range(len(data) - 3):
            pattern = data[i:i+4]
            pattern_hex = pattern.hex()
            patterns[pattern_hex] = patterns.get(pattern_hex, 0) + 1
        
        # æ‰¾å‡ºæœ€å¸¸è§çš„æ¨¡å¼
        if patterns:
            most_common = max(patterns.items(), key=lambda x: x[1])
            if most_common[1] > 1:
                padding_info['pattern_analysis'] = {
                    'most_common_pattern': most_common[0],
                    'pattern_frequency': most_common[1],
                    'total_patterns': len(patterns)
                }
    
    # 7. åè®®ç‰¹å¾åˆ†æ
    if padding_info['padding_detected']:
        # åŸºäºå¡«å……ç±»å‹æ¨æ–­åè®®
        protocol_mapping = {
            'PKCS#7': ['TLS 1.0-1.2', 'SSL 3.0', 'AES-CBC', 'IPSec ESP'],
            'Zero Padding': ['IPSec AH', 'Custom Protocol', 'Legacy Systems'],
            'ANSI X9.23': ['Legacy SSL', 'Financial Systems', 'Custom Crypto'],
            'ISO 10126': ['TLS 1.1+', 'Modern Cryptography', 'High Security Systems']
        }
        
        padding_type = padding_info['padding_type']
        if padding_type in protocol_mapping:
            padding_info['protocol_indicators'].extend(protocol_mapping[padding_type])
        
        # å»é‡
        padding_info['protocol_indicators'] = list(set(padding_info['protocol_indicators']))
    
    return padding_info

def analyze_protocol_characteristics(padding_analysis, tls_data, tcp_data):
    """åŸºäºå¡«å……åˆ†ææ¨æ–­åè®®ç‰¹å¾"""
    characteristics = {
        'likely_protocols': [],
        'encryption_mode': 'Unknown',
        'security_level': 'Unknown',
        'implementation_hints': []
    }
    
    if not padding_analysis.get('padding_detected'):
        # æ— å¡«å……å¯èƒ½è¡¨ç¤ºæµå¯†ç æˆ–AEADæ¨¡å¼
        characteristics.update({
            'likely_protocols': ['TLS 1.3', 'ChaCha20-Poly1305', 'AES-GCM'],
            'encryption_mode': 'Stream/AEAD',
            'security_level': 'Modern'
        })
        return characteristics
    
    padding_type = padding_analysis.get('padding_type')
    padding_bytes = padding_analysis.get('padding_bytes', 0)
    
    # åŸºäºå¡«å……ç±»å‹åˆ†æ
    if padding_type == 'PKCS#7':
        if padding_bytes <= 16:
            characteristics.update({
                'likely_protocols': ['TLS 1.0-1.2', 'AES-CBC'],
                'encryption_mode': 'CBC',
                'security_level': 'Standard'
            })
            
            # åŸºäºå¡«å……é•¿åº¦è¿›ä¸€æ­¥åˆ†æ
            if padding_bytes == 16:
                characteristics['implementation_hints'].append('Full block padding - possible timing attack mitigation')
            elif padding_bytes == 1:
                characteristics['implementation_hints'].append('Minimal padding - efficiency focused')
    
    elif padding_type == 'ISO 10126':
        characteristics.update({
            'likely_protocols': ['TLS 1.1+', 'Modern SSL'],
            'encryption_mode': 'CBC with random padding',
            'security_level': 'Enhanced',
            'implementation_hints': ['Random padding for side-channel resistance']
        })
    
    # åŸºäºTLSè®°å½•é•¿åº¦åˆ†æ
    if tls_data and 'encrypted_length' in tls_data:
        record_length = tls_data['encrypted_length']
        if record_length == 16384:  # æœ€å¤§TLSè®°å½•
            characteristics['implementation_hints'].append('Maximum TLS record size - bulk data transfer')
        elif record_length < 100:
            characteristics['implementation_hints'].append('Small record - likely control/handshake data')
    
    return characteristics

def extract_text_content(data):
    """æå–æ•°æ®ä¸­çš„æ–‡æœ¬å†…å®¹"""
    text_content = {
        'readable_text': '',
        'text_ratio': 0,
        'contains_html': False,
        'contains_json': False,
        'contains_image_data': False
    }
    
    if not data:
        return text_content
    
    # è½¬æ¢ä¸ºå­—èŠ‚
    if isinstance(data, str):
        try:
            if data.startswith('0x'):
                data = data[2:]
            data = bytes.fromhex(data.replace(':', '').replace(' ', ''))
        except:
            data = data.encode('utf-8')
    
    # å°è¯•è§£ç ä¸ºæ–‡æœ¬
    try:
        text = data.decode('utf-8', errors='ignore')
        # è®¡ç®—å¯è¯»å­—ç¬¦æ¯”ä¾‹
        printable_chars = sum(1 for c in text if c.isprintable())
        text_content['text_ratio'] = printable_chars / len(text) if text else 0
        
        # æå–å¯è¯»æ–‡æœ¬
        readable_text = ''.join(c for c in text if c.isprintable())
        text_content['readable_text'] = readable_text[:500]  # é™åˆ¶é•¿åº¦
        
        # æ£€æµ‹å†…å®¹ç±»å‹
        text_lower = readable_text.lower()
        text_content['contains_html'] = '<html' in text_lower or '<!doctype' in text_lower
        text_content['contains_json'] = ('{' in text and '}' in text) or ('[' in text and ']' in text)
        
        # æ£€æµ‹å›¾ç‰‡æ•°æ®ç‰¹å¾
        image_signatures = [b'\xff\xd8\xff', b'\x89PNG', b'GIF8', b'RIFF']
        text_content['contains_image_data'] = any(sig in data for sig in image_signatures)
        
    except Exception as e:
        pass
    
    return text_content

def analyze_fragmentation(packets):
    """åˆ†ææ•°æ®åˆ†ç‰‡æƒ…å†µ"""
    fragmentation_info = {
        'total_fragments': len(packets),
        'fragment_sizes': [],
        'size_variance': 0,
        'reassembly_order': [],
        'gaps_detected': False
    }
    
    for packet in packets:
        if hasattr(packet, 'tcp'):
            seq_num = safe_int_convert(packet.tcp.seq)
            payload_len = safe_int_convert(getattr(packet.tcp, 'len', 0))
            fragmentation_info['fragment_sizes'].append(payload_len)
            fragmentation_info['reassembly_order'].append(seq_num)
    
    if fragmentation_info['fragment_sizes']:
        fragmentation_info['size_variance'] = np.var(fragmentation_info['fragment_sizes'])
    
    # æ£€æµ‹åºåˆ—å·é—´éš™
    if len(fragmentation_info['reassembly_order']) > 1:
        sorted_seqs = sorted(fragmentation_info['reassembly_order'])
        for i in range(1, len(sorted_seqs)):
            if sorted_seqs[i] - sorted_seqs[i-1] > max(fragmentation_info['fragment_sizes']):
                fragmentation_info['gaps_detected'] = True
                break
    
    return fragmentation_info

def deep_packet_analysis(cap_file, target_name):
    """æ·±åº¦æ•°æ®åŒ…åˆ†æ"""
    # è·å–å¯¹åº”çš„tsharkè¿‡æ»¤å™¨
    display_filter = name_package_dict.get(target_name, "")
    print(f"ä½¿ç”¨è¿‡æ»¤å™¨: {display_filter}")
    
    # ä½¿ç”¨tsharkè·¯å¾„å’Œè¿‡æ»¤å™¨
    cap = pyshark.FileCapture(cap_file, tshark_path=tshark_path, display_filter=display_filter)
    analysis_results = {
        'timestamp': datetime.now().isoformat(),
        'total_packets': 0,
        'detailed_analysis': [],
        'encryption_analysis': {
            'entropy_comparison': [],
            'length_expansion': [],
            'padding_analysis': [],
            'fragmentation_patterns': []
        },
        'content_analysis': {
            'text_extraction': [],
            'data_types': Counter(),
            'compression_indicators': []
        }
    }
    
    packets_by_stream = {}
    
    for packet in cap:
        analysis_results['total_packets'] += 1
        
        packet_analysis = {
            'packet_number': safe_int_convert(packet.number),
            'timestamp': str(packet.sniff_time),
            'layers': {}
        }
        
        # åˆ†æå„å±‚æ•°æ®
        for layer in packet.layers:
            layer_name = layer.layer_name
            layer_analysis = {'raw_data': None, 'decoded_data': None}
            
            if layer_name == 'tcp':
                tcp_data = {
                    'sequence_number': safe_int_convert(getattr(layer, 'seq', 0)),
                    'payload_length': safe_int_convert(getattr(layer, 'len', 0)),
                    'flags': str(layer.flags_str) if hasattr(layer, 'flags_str') else '',
                    'window_size': safe_int_convert(getattr(layer, 'window_size_value', 0))
                }
                
                # æå–TCPè½½è·
                if hasattr(layer, 'payload'):
                    tcp_data['payload_hex'] = str(layer.payload)
                    tcp_data['payload_entropy'] = calculate_entropy(str(layer.payload))
                
                layer_analysis['decoded_data'] = tcp_data
                
            # åœ¨ deep_packet_analysis å‡½æ•°çš„ TLS å±‚åˆ†æéƒ¨åˆ†ä¿®æ”¹
            elif layer_name == 'tls':
                tls_data = {
                    'record_type': safe_int_convert(getattr(layer, 'record_content_type', 0)),
                    'version': str(layer.record_version) if hasattr(layer, 'record_version') else '',
                    'encrypted_length': safe_int_convert(getattr(layer, 'record_length', 0))
                }
                
                # æå–åŠ å¯†æ•°æ®
                if hasattr(layer, 'app_data'):
                    encrypted_data = str(layer.app_data)
                    tls_data['encrypted_data_hex'] = encrypted_data
                    tls_data['encrypted_entropy'] = calculate_entropy(encrypted_data)
                    
                    # ä½¿ç”¨å¢å¼ºçš„å¡«å……åˆ†æ
                    tls_data['padding_analysis'] = analyze_advanced_padding(encrypted_data)
                    
                    # åè®®ç‰¹å¾åˆ†æ
                    tcp_layer_data = packet_analysis['layers'].get('tcp', {}).get('decoded_data', {})
                    tls_data['protocol_characteristics'] = analyze_protocol_characteristics(
                        tls_data['padding_analysis'], tls_data, tcp_layer_data
                    )
                
                layer_analysis['decoded_data'] = tls_data
                
            elif layer_name == 'http2':
                http2_data = {
                    'stream_id': safe_int_convert(getattr(layer, 'streamid', 0)),
                    'frame_type': safe_int_convert(getattr(layer, 'type', 0)),
                    'frame_length': safe_int_convert(getattr(layer, 'length', 0)),
                    'flags': safe_int_convert(getattr(layer, 'flags', 0))
                }
                
                # æå–HTTP2æ•°æ®
                if hasattr(layer, 'data'):
                    plaintext_data = str(layer.data)
                    http2_data['plaintext_hex'] = plaintext_data
                    http2_data['plaintext_entropy'] = calculate_entropy(plaintext_data)
                    http2_data['text_content'] = extract_text_content(plaintext_data)
                    http2_data['padding_analysis'] = analyze_padding(plaintext_data)
                
                # æå–å¤´éƒ¨ä¿¡æ¯
                headers = {}
                for field_name in layer.field_names:
                    if field_name.startswith('headers_'):
                        header_name = field_name.replace('headers_', '')
                        headers[header_name] = str(getattr(layer, field_name))
                
                http2_data['headers'] = headers
                layer_analysis['decoded_data'] = http2_data
            
            packet_analysis['layers'][layer_name] = layer_analysis
        
        # è¿›è¡Œå¯†æ–‡æ˜æ–‡å¯¹æ¯”åˆ†æ
        if 'tls' in packet_analysis['layers'] and 'http2' in packet_analysis['layers']:
            tls_layer = packet_analysis['layers']['tls']['decoded_data']
            http2_layer = packet_analysis['layers']['http2']['decoded_data']
            
            comparison = {
                'packet_number': packet_analysis['packet_number'],
                'plaintext_length': http2_layer.get('frame_length', 0),
                'ciphertext_length': tls_layer.get('encrypted_length', 0),
                'length_expansion_ratio': 0,
                'entropy_difference': 0,
                'compression_detected': False
            }
            
            if comparison['plaintext_length'] > 0:
                comparison['length_expansion_ratio'] = comparison['ciphertext_length'] / comparison['plaintext_length']
            
            # ç†µå€¼å¯¹æ¯”
            plaintext_entropy = http2_layer.get('plaintext_entropy', 0)
            ciphertext_entropy = tls_layer.get('encrypted_entropy', 0)
            comparison['entropy_difference'] = ciphertext_entropy - plaintext_entropy
            
            # å‹ç¼©æ£€æµ‹
            if comparison['length_expansion_ratio'] < 1.0:
                comparison['compression_detected'] = True
            
            analysis_results['encryption_analysis']['entropy_comparison'].append(comparison)
        
        analysis_results['detailed_analysis'].append(packet_analysis)
        
        # æŒ‰æµåˆ†ç»„
        if 'http2' in packet_analysis['layers']:
            stream_id = packet_analysis['layers']['http2']['decoded_data'].get('stream_id', 0)
            if stream_id not in packets_by_stream:
                packets_by_stream[stream_id] = []
            packets_by_stream[stream_id].append(packet)
    
    # åˆ†æåˆ†ç‰‡æ¨¡å¼
    for stream_id, stream_packets in packets_by_stream.items():
        if len(stream_packets) > 1:
            frag_analysis = analyze_fragmentation(stream_packets)
            frag_analysis['stream_id'] = stream_id
            analysis_results['encryption_analysis']['fragmentation_patterns'].append(frag_analysis)
    
    cap.close()
    return analysis_results

def create_deep_analysis_charts(name, analysis_data, output_dir):
    """åˆ›å»ºä¼˜åŒ–çš„æ·±åº¦åˆ†æå›¾è¡¨"""
    fig = plt.figure(figsize=(16, 12))
    
    # 1. ç†µå€¼å¯¹æ¯”åˆ†æ - æ ¸å¿ƒåŠ å¯†åˆ†æ
    ax1 = plt.subplot(2, 3, 1)
    entropy_data = analysis_data['encryption_analysis']['entropy_comparison']
    if entropy_data:
        packets = [d['packet_number'] for d in entropy_data]
        plaintext_entropies = []
        ciphertext_entropies = []
        
        # åœ¨create_deep_analysis_chartså‡½æ•°ä¸­
        for packet_data in analysis_data['detailed_analysis']:
            if 'tcp' in packet_data['layers'] and 'tls' in packet_data['layers']:
                tcp_entropy = packet_data['layers']['tcp']['decoded_data'].get('payload_entropy', 0)
                tls_entropy = packet_data['layers']['tls']['decoded_data'].get('encrypted_entropy', 0)
                plaintext_entropies.append(tcp_entropy)  # ä½¿ç”¨TCPè½½è·ç†µå€¼
                ciphertext_entropies.append(tls_entropy)
        
        if plaintext_entropies and ciphertext_entropies:
            x = np.arange(len(packets))
            width = 0.35
            ax1.bar(x - width/2, plaintext_entropies, width, label='æ˜æ–‡ç†µå€¼', alpha=0.8, color='lightblue')
            ax1.bar(x + width/2, ciphertext_entropies, width, label='å¯†æ–‡ç†µå€¼', alpha=0.8, color='lightcoral')
            ax1.set_title('æ˜æ–‡vså¯†æ–‡ç†µå€¼å¯¹æ¯”', fontsize=12, fontweight='bold')
            ax1.set_xlabel('æ•°æ®åŒ…')
            ax1.set_ylabel('ç†µå€¼')
            ax1.legend()
            ax1.set_xticks(x)
            ax1.set_xticklabels([f'#{p}' for p in packets], rotation=45)
            ax1.grid(True, alpha=0.3)
    
    # 2. é•¿åº¦æ‰©å¼ åˆ†æ - åŠ å¯†å¼€é”€åˆ†æ
    ax2 = plt.subplot(2, 3, 2)
    if entropy_data:
        expansion_ratios = [d['length_expansion_ratio'] for d in entropy_data if d['length_expansion_ratio'] > 0]
        packet_nums = [d['packet_number'] for d in entropy_data if d['length_expansion_ratio'] > 0]
        
        if expansion_ratios:
            colors = ['red' if r > 1.2 else 'orange' if r > 1.0 else 'green' for r in expansion_ratios]
            bars = ax2.bar(range(len(expansion_ratios)), expansion_ratios, color=colors, alpha=0.7)
            ax2.set_title('å¯†æ–‡é•¿åº¦æ‰©å¼ ç‡', fontsize=12, fontweight='bold')
            ax2.set_xlabel('æ•°æ®åŒ…')
            ax2.set_ylabel('æ‰©å¼ ç‡')
            ax2.axhline(y=1.0, color='black', linestyle='--', alpha=0.5, label='åŸºå‡†çº¿')
            ax2.set_xticks(range(len(packet_nums)))
            ax2.set_xticklabels([f'#{p}' for p in packet_nums], rotation=45)
            ax2.legend()
            ax2.grid(True, alpha=0.3)
    
    # 3. å¡«å……ç±»å‹åˆ†æ
    ax3 = plt.subplot(2, 3, 3)
    padding_types = Counter()
    
    for packet_data in analysis_data['detailed_analysis']:
        for layer_name, layer_data in packet_data['layers'].items():
            if 'decoded_data' in layer_data and layer_data['decoded_data']:
                padding_info = layer_data['decoded_data'].get('padding_analysis')
                if padding_info:
                    if padding_info.get('padding_detected'):
                        padding_type = padding_info.get('padding_type', 'Unknown')
                        padding_types[padding_type] += 1
                    else:
                        padding_types['No Padding'] += 1
    
    if padding_types:
        labels = list(padding_types.keys())
        sizes = list(padding_types.values())
        colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']
        ax3.pie(sizes, labels=labels, colors=colors[:len(labels)], autopct='%1.1f%%', startangle=90)
        ax3.set_title('å¡«å……ç±»å‹åˆ†å¸ƒ', fontsize=12, fontweight='bold')
    
    # 4. æ•°æ®åˆ†ç‰‡å¤§å°åˆ†å¸ƒ
    ax4 = plt.subplot(2, 3, 4)
    fragment_sizes = []
    for frag_pattern in analysis_data['encryption_analysis']['fragmentation_patterns']:
        if frag_pattern.get('fragment_sizes'):
            fragment_sizes.extend(frag_pattern['fragment_sizes'])
    
    if fragment_sizes:
        ax4.hist(fragment_sizes, bins=min(20, len(set(fragment_sizes))), alpha=0.7, color='skyblue', edgecolor='black')
        ax4.set_title('æ•°æ®åˆ†ç‰‡å¤§å°åˆ†å¸ƒ', fontsize=12, fontweight='bold')
        ax4.set_xlabel('åˆ†ç‰‡å¤§å° (å­—èŠ‚)')
        ax4.set_ylabel('é¢‘æ¬¡')
        ax4.grid(True, alpha=0.3)
    
    # 5. æ”¹è¿›çš„æ•°æ®æµæ—¶é—´çº¿
    ax5 = plt.subplot(2, 3, 5)
    timestamps = []
    data_sizes = []
    packet_numbers = []
    
    for packet_data in analysis_data['detailed_analysis']:
        try:
            # ä½¿ç”¨æ•°æ®åŒ…åºå·ä½œä¸ºæ—¶é—´è½´ï¼Œæ›´ç¨³å®š
            packet_numbers.append(packet_data['packet_number'])
            
            total_size = 0
            for layer_data in packet_data['layers'].values():
                if 'decoded_data' in layer_data and layer_data['decoded_data']:
                    # å°è¯•å¤šç§å¤§å°å­—æ®µ
                    size_fields = ['frame_length', 'length', 'data_length', 'payload_length']
                    for field in size_fields:
                        if field in layer_data['decoded_data']:
                            size_val = layer_data['decoded_data'][field]
                            if isinstance(size_val, (int, float)) and size_val > 0:
                                total_size += size_val
                                break
            
            if total_size > 0:
                data_sizes.append(total_size)
            else:
                data_sizes.append(0)
        except Exception as e:
            packet_numbers.append(packet_data.get('packet_number', len(packet_numbers)))
            data_sizes.append(0)
    
    if packet_numbers and data_sizes and any(s > 0 for s in data_sizes):
        ax5.plot(packet_numbers, data_sizes, marker='o', linewidth=2, markersize=4, color='blue')
        ax5.set_title('æ•°æ®åŒ…å¤§å°å˜åŒ–è¶‹åŠ¿', fontsize=12, fontweight='bold')
        ax5.set_xlabel('æ•°æ®åŒ…åºå·')
        ax5.set_ylabel('æ•°æ®å¤§å° (å­—èŠ‚)')
        ax5.grid(True, alpha=0.3)
    
    # 6. å‹ç¼©/æ‰©å¼ æ•ˆæœåˆ†æ
    ax6 = plt.subplot(2, 3, 6)
    if entropy_data:
        compression_stats = {'å‹ç¼©': 0, 'æ‰©å¼ ': 0, 'æ— å˜åŒ–': 0}
        
        for comp_data in entropy_data:
            ratio = comp_data['length_expansion_ratio']
            if ratio < 0.95:
                compression_stats['å‹ç¼©'] += 1
            elif ratio > 1.05:
                compression_stats['æ‰©å¼ '] += 1
            else:
                compression_stats['æ— å˜åŒ–'] += 1
        
        if any(compression_stats.values()):
            labels = list(compression_stats.keys())
            sizes = list(compression_stats.values())
            colors = ['green', 'red', 'gray']
            wedges, texts, autotexts = ax6.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
            ax6.set_title('æ•°æ®å‹ç¼©/æ‰©å¼ åˆ†å¸ƒ', fontsize=12, fontweight='bold')
            
            # ç¾åŒ–é¥¼å›¾æ–‡å­—
            for autotext in autotexts:
                autotext.set_color('white')
                autotext.set_fontweight('bold')
    
    plt.tight_layout(pad=3.0)
    chart_file = os.path.join(output_dir, f'{name}_æ·±åº¦åˆ†æå›¾è¡¨.png')
    plt.savefig(chart_file, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    return chart_file

def generate_deep_analysis_report(name, analysis_data, output_dir):
    """ç”Ÿæˆæ·±åº¦åˆ†ææŠ¥å‘Š"""
    report = []
    
    # æŠ¥å‘Šæ ‡é¢˜
    report.append("="*80)
    report.append("ç½‘ç»œæ•°æ®åŒ…æ·±åº¦åˆ†ææŠ¥å‘Š")
    report.append("å¯†æ–‡ä¸æ˜æ–‡ç‰¹å¾å¯¹æ¯”åˆ†æ")
    report.append("="*80)
    report.append(f"åˆ†ææ—¶é—´: {analysis_data['timestamp']}")
    report.append(f"æ€»æ•°æ®åŒ…æ•°: {analysis_data['total_packets']}")
    report.append("")
    
    # åˆå§‹åŒ–å˜é‡
    avg_expansion = 0
    avg_entropy_diff = 0
    compression_count = 0
    
    # åŠ å¯†åˆ†ææ¦‚è§ˆ
    entropy_data = analysis_data['encryption_analysis']['entropy_comparison']
    report.append("ğŸ” åŠ å¯†ç‰¹å¾åˆ†ææ¦‚è§ˆ")
    report.append("-"*50)
    
    if entropy_data:
        avg_expansion = np.mean([d['length_expansion_ratio'] for d in entropy_data if d['length_expansion_ratio'] > 0])
        avg_entropy_diff = np.mean([d['entropy_difference'] for d in entropy_data])
        compression_count = sum(1 for d in entropy_data if d['compression_detected'])
        
        report.append(f"å¹³å‡é•¿åº¦æ‰©å¼ ç‡: {avg_expansion:.3f}")
        report.append(f"å¹³å‡ç†µå€¼å¢åŠ : {avg_entropy_diff:.3f}")
        report.append(f"æ£€æµ‹åˆ°å‹ç¼©çš„æ•°æ®åŒ…: {compression_count}/{len(entropy_data)}")
        report.append("")
    
    # è¯¦ç»†åˆ†æè¡¨æ ¼
    report.append("ğŸ“Š å¯†æ–‡æ˜æ–‡å¯¹æ¯”è¯¦ç»†åˆ†æ")
    report.append("-"*50)
    report.append(f"{'åŒ…å·':<8} {'æ˜æ–‡é•¿åº¦':<10} {'å¯†æ–‡é•¿åº¦':<10} {'æ‰©å¼ ç‡':<8} {'ç†µå€¼å·®':<8} {'å‹ç¼©':<6}")
    report.append("-"*60)
    
    for data in entropy_data:
        compression_mark = "æ˜¯" if data['compression_detected'] else "å¦"
        report.append(f"{data['packet_number']:<8} {data['plaintext_length']:<10} {data['ciphertext_length']:<10} "
                     f"{data['length_expansion_ratio']:<8.3f} {data['entropy_difference']:<8.3f} {compression_mark:<6}")
    
    # æ–°å¢ï¼šæ•°æ®åŒ…å­—èŠ‚ç»Ÿè®¡è¯¦ç»†åˆ†æ
    report.append("\nğŸ“ æ•°æ®åŒ…å­—èŠ‚ç»Ÿè®¡è¯¦ç»†åˆ†æ")
    report.append("-"*80)
    report.append(f"{'åŒ…å·':<8} {'æ€»å­—èŠ‚æ•°':<10} {'å¡«å……å­—èŠ‚':<10} {'å†…å®¹å­—èŠ‚':<10} {'å¡«å……ç‡':<8} {'å¡«å……ç±»å‹':<12}")
    report.append("-"*80)
    
    total_bytes_sum = 0
    total_padding_sum = 0
    total_content_sum = 0
    
    for packet_data in analysis_data['detailed_analysis']:
        packet_num = packet_data['packet_number']
        total_bytes = 0
        padding_bytes = 0
        content_bytes = 0
        padding_type = "æ— å¡«å……"
        
        # è®¡ç®—æ€»å­—èŠ‚æ•°ï¼ˆä»TCPå±‚è·å–è½½è·é•¿åº¦ï¼‰
        if 'tcp' in packet_data['layers'] and packet_data['layers']['tcp']['decoded_data']:
            tcp_data = packet_data['layers']['tcp']['decoded_data']
            total_bytes = tcp_data.get('payload_length', 0)
        
        # è®¡ç®—å¡«å……å­—èŠ‚æ•°ï¼ˆä»TLSå±‚è·å–å¡«å……ä¿¡æ¯ï¼‰
        if 'tls' in packet_data['layers'] and packet_data['layers']['tls']['decoded_data']:
            tls_data = packet_data['layers']['tls']['decoded_data']
            padding_info = tls_data.get('padding_analysis', {})
            if padding_info.get('padding_detected'):
                padding_bytes = padding_info.get('padding_bytes', 0)
                padding_type = padding_info.get('padding_type', 'æœªçŸ¥')
        
        # è®¡ç®—å†…å®¹å­—èŠ‚æ•°
        content_bytes = max(0, total_bytes - padding_bytes)
        
        # è®¡ç®—å¡«å……ç‡
        padding_ratio = (padding_bytes / total_bytes * 100) if total_bytes > 0 else 0
        
        # ç´¯è®¡ç»Ÿè®¡
        total_bytes_sum += total_bytes
        total_padding_sum += padding_bytes
        total_content_sum += content_bytes
        
        # æ·»åŠ åˆ°æŠ¥å‘Š
        report.append(f"{packet_num:<8} {total_bytes:<10} {padding_bytes:<10} {content_bytes:<10} "
                     f"{padding_ratio:<8.1f}% {padding_type:<12}")
    
    # æ·»åŠ ç»Ÿè®¡æ±‡æ€»
    report.append("-"*80)
    report.append(f"{'æ±‡æ€»':<8} {total_bytes_sum:<10} {total_padding_sum:<10} {total_content_sum:<10} "
                 f"{(total_padding_sum/total_bytes_sum*100 if total_bytes_sum > 0 else 0):<8.1f}% {'æ€»è®¡':<12}")
    report.append("")
    
    # å­—èŠ‚ç»Ÿè®¡åˆ†æ
    report.append("ğŸ“ˆ å­—èŠ‚ç»Ÿè®¡åˆ†æ")
    report.append("-"*50)
    report.append(f"æ€»ä¼ è¾“å­—èŠ‚æ•°: {total_bytes_sum:,} å­—èŠ‚")
    report.append(f"æ€»å¡«å……å­—èŠ‚æ•°: {total_padding_sum:,} å­—èŠ‚")
    report.append(f"æ€»å†…å®¹å­—èŠ‚æ•°: {total_content_sum:,} å­—èŠ‚")
    report.append(f"å¹³å‡å¡«å……ç‡: {(total_padding_sum/total_bytes_sum*100 if total_bytes_sum > 0 else 0):.2f}%")
    
    if total_bytes_sum > 0:
        efficiency = (total_content_sum / total_bytes_sum) * 100
        report.append(f"ä¼ è¾“æ•ˆç‡: {efficiency:.2f}% (å†…å®¹å­—èŠ‚/æ€»å­—èŠ‚)")
        
        if efficiency > 90:
            report.append("âœ… ä¼ è¾“æ•ˆç‡ä¼˜ç§€ï¼Œå¡«å……å¼€é”€è¾ƒå°")
        elif efficiency > 80:
            report.append("âš ï¸  ä¼ è¾“æ•ˆç‡è‰¯å¥½ï¼Œä½†å¯è¿›ä¸€æ­¥ä¼˜åŒ–å¡«å……")
        else:
            report.append("âŒ ä¼ è¾“æ•ˆç‡è¾ƒä½ï¼Œå¡«å……å¼€é”€è¿‡å¤§")
    
    report.append("")
    
    # å¡«å……åˆ†æ
    report.append("\nğŸ”§ æ•°æ®å¡«å……åˆ†æ")
    report.append("-"*50)
    
    padding_stats = {'PKCS#7': 0, 'Zero Padding': 0, 'No Padding': 0}
    total_padding_bytes = 0
    
    for packet_data in analysis_data['detailed_analysis']:
        for layer_data in packet_data['layers'].values():
            if 'decoded_data' in layer_data and layer_data['decoded_data']:
                padding_info = layer_data['decoded_data'].get('padding_analysis')
                if padding_info:
                    if padding_info.get('padding_detected'):
                        padding_type = padding_info.get('padding_type', 'Unknown')
                        padding_stats[padding_type] = padding_stats.get(padding_type, 0) + 1
                        total_padding_bytes += padding_info.get('padding_bytes', 0)
                    else:
                        padding_stats['No Padding'] += 1
    
    for padding_type, count in padding_stats.items():
        report.append(f"{padding_type}: {count} æ¬¡")
    report.append(f"æ€»å¡«å……å­—èŠ‚æ•°: {total_padding_bytes}")
    
    # å†…å®¹ç±»å‹åˆ†æ
    report.append("\nğŸ“„ æ•°æ®å†…å®¹åˆ†æ")
    report.append("-"*50)
    
    content_analysis = []
    for packet_data in analysis_data['detailed_analysis']:
        if 'http2' in packet_data['layers']:
            text_content = packet_data['layers']['http2']['decoded_data'].get('text_content', {})
            if text_content.get('readable_text'):
                content_analysis.append({
                    'packet': packet_data['packet_number'],
                    'text_ratio': text_content.get('text_ratio', 0),
                    'content_type': 'HTML' if text_content.get('contains_html') else 
                                   'JSON' if text_content.get('contains_json') else
                                   'Image' if text_content.get('contains_image_data') else 'Text',
                    'sample_text': text_content.get('readable_text', '')[:100]
                })
    
    report.append(f"{'åŒ…å·':<8} {'å¯è¯»ç‡':<8} {'ç±»å‹':<8} {'å†…å®¹æ ·æœ¬':<50}")
    report.append("-"*80)
    for content in content_analysis[:10]:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
        report.append(f"{content['packet']:<8} {content['text_ratio']:<8.2f} {content['content_type']:<8} "
                     f"{content['sample_text']:<50}")
    
    # åˆ†ç‰‡åˆ†æ
    report.append("\nğŸ§© æ•°æ®åˆ†ç‰‡ä¸é‡ç»„åˆ†æ")
    report.append("-"*50)
    
    for frag_pattern in analysis_data['encryption_analysis']['fragmentation_patterns']:
        report.append(f"æµ ID {frag_pattern['stream_id']}:")
        report.append(f"  åˆ†ç‰‡æ•°é‡: {frag_pattern['total_fragments']}")
        report.append(f"  å¤§å°æ–¹å·®: {frag_pattern['size_variance']:.2f}")
        report.append(f"  æ£€æµ‹åˆ°é—´éš™: {'æ˜¯' if frag_pattern['gaps_detected'] else 'å¦'}")
        if frag_pattern['fragment_sizes']:
            avg_size = np.mean(frag_pattern['fragment_sizes'])
            report.append(f"  å¹³å‡åˆ†ç‰‡å¤§å°: {avg_size:.2f} å­—èŠ‚")
        report.append("")
    
    # å®‰å…¨æ€§è¯„ä¼°
    report.append("ğŸ›¡ï¸ å®‰å…¨æ€§è¯„ä¼°")
    report.append("-"*50)
    
    # ç†µå€¼è¯„ä¼°
    high_entropy_count = 0
    low_entropy_count = 0
    
    for packet_data in analysis_data['detailed_analysis']:
        if 'tls' in packet_data['layers']:
            entropy = packet_data['layers']['tls']['decoded_data'].get('encrypted_entropy', 0)
            if entropy > 7.5:  # é«˜ç†µå€¼é˜ˆå€¼
                high_entropy_count += 1
            elif entropy < 6.0:  # ä½ç†µå€¼é˜ˆå€¼
                low_entropy_count += 1
    
    report.append(f"é«˜ç†µå€¼å¯†æ–‡åŒ… (>7.5): {high_entropy_count}")
    report.append(f"ä½ç†µå€¼å¯†æ–‡åŒ… (<6.0): {low_entropy_count}")
    
    if high_entropy_count > low_entropy_count:
        report.append("âœ… åŠ å¯†è´¨é‡è‰¯å¥½ï¼Œç†µå€¼åˆ†å¸ƒæ­£å¸¸")
    else:
        report.append("âš ï¸  éƒ¨åˆ†å¯†æ–‡ç†µå€¼åä½ï¼Œå¯èƒ½å­˜åœ¨æ¨¡å¼")
    
    # å»ºè®®
    report.append("\nğŸ’¡ ä¼˜åŒ–å»ºè®®")
    report.append("-"*50)
    
    if avg_expansion > 1.2:
        report.append("â€¢ è€ƒè™‘ä¼˜åŒ–åŠ å¯†ç®—æ³•æˆ–å‡å°‘å¡«å……å¼€é”€")
    if compression_count > 0:
        report.append("â€¢ æ£€æµ‹åˆ°æ•°æ®å‹ç¼©ï¼Œå»ºè®®åˆ†æå‹ç¼©ç®—æ³•æ•ˆæœ")
    if total_padding_bytes > 100:
        report.append("â€¢ å¡«å……å­—èŠ‚è¾ƒå¤šï¼Œå¯è€ƒè™‘ä¼˜åŒ–å¡«å……ç­–ç•¥")
    
    report.append("\n" + "="*80)
    report.append("æ·±åº¦åˆ†ææŠ¥å‘Šå®Œæˆ")
    report.append("="*80)
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = os.path.join(output_dir, f'{name}_æ·±åº¦åˆ†ææŠ¥å‘Š.txt')
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report))
    
    # ä¿å­˜JSONæ•°æ®
    json_file = os.path.join(output_dir, f'{name}_æ·±åº¦åˆ†ææ•°æ®.json')
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(analysis_data, f, ensure_ascii=False, indent=2, default=str)
    
    return report_file, json_file

def main(target_name):
    """ä¸»å‡½æ•°"""
    # å¯é€‰æ‹©çš„åˆ†æç›®æ ‡
    available_targets = list(name_package_dict.keys())
    print("å¯ç”¨çš„æ·±åº¦åˆ†æç›®æ ‡:")
    for i, target in enumerate(available_targets, 1):
        print(f"{i}. {target}")
        
    print(f"\nå½“å‰æ·±åº¦åˆ†æç›®æ ‡: {target_name}")
    print(f"å¯¹åº”çš„è¿‡æ»¤å™¨: {name_package_dict[target_name]}")
    
    # æ„å»ºpcapæ–‡ä»¶è·¯å¾„
    current_dir = os.path.dirname(os.path.abspath(__file__))
    pcap_file = os.path.join(current_dir, f'{target_name}.pcapng')
    
    if not os.path.exists(pcap_file):
        print(f"æ–‡ä»¶ä¸å­˜åœ¨: {pcap_file}")
        return
    
    print("å¼€å§‹æ·±åº¦åˆ†æ...")
    
    try:
        # æ‰§è¡Œæ·±åº¦åˆ†æï¼Œä¼ å…¥target_nameå‚æ•°ä»¥ä½¿ç”¨å¯¹åº”çš„è¿‡æ»¤å™¨
        analysis_data = deep_packet_analysis(pcap_file, target_name)
        
        # ç”Ÿæˆå›¾è¡¨
        print("ç”Ÿæˆåˆ†æå›¾è¡¨...")
        chart_file = create_deep_analysis_charts(target_name, analysis_data, current_dir)
        print(f"å›¾è¡¨å·²ä¿å­˜: {os.path.basename(chart_file)}")
        
        # ç”ŸæˆæŠ¥å‘Š
        print("ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        report_file, json_file = generate_deep_analysis_report(target_name, analysis_data, current_dir)
        print(f"æŠ¥å‘Šå·²ä¿å­˜: {os.path.basename(report_file)}")
        print(f"æ•°æ®å·²ä¿å­˜: {os.path.basename(json_file)}")
        
        print("\næ·±åº¦åˆ†æå®Œæˆï¼")
        
    except Exception as e:
        print(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main('äº”å¤§æ¹–')
    main('å¥¥ç‰¹æ›¼')
    main('å‡é¢éª‘å£«')
    main('å°¼äºšåŠ æ‹‰ç€‘å¸ƒ')
    main('å­™ç­–')

# åœ¨ create_deep_analysis_charts å‡½æ•°ä¸­æ·»åŠ æ–°çš„å¡«å……åˆ†æå›¾è¡¨
def create_padding_analysis_chart(analysis_data):
    """åˆ›å»ºè¯¦ç»†çš„å¡«å……åˆ†æå›¾è¡¨"""
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. å¡«å……ç±»å‹åˆ†å¸ƒ
    padding_types = Counter()
    protocol_indicators = Counter()
    
    for packet_data in analysis_data['detailed_analysis']:
        for layer_data in packet_data['layers'].values():
            if 'decoded_data' in layer_data and layer_data['decoded_data']:
                padding_info = layer_data['decoded_data'].get('padding_analysis', {})
                if padding_info.get('padding_detected'):
                    padding_type = padding_info.get('padding_type', 'Unknown')
                    padding_types[padding_type] += 1
                    
                    # æ”¶é›†åè®®æŒ‡ç¤ºå™¨
                    indicators = padding_info.get('protocol_indicators', [])
                    for indicator in indicators:
                        protocol_indicators[indicator] += 1
    
    # ç»˜åˆ¶å¡«å……ç±»å‹é¥¼å›¾
    if padding_types:
        ax1.pie(padding_types.values(), labels=padding_types.keys(), autopct='%1.1f%%')
        ax1.set_title('å¡«å……ç±»å‹åˆ†å¸ƒ')
    
    # 2. åè®®æŒ‡ç¤ºå™¨åˆ†å¸ƒ
    if protocol_indicators:
        top_protocols = dict(protocol_indicators.most_common(8))
        ax2.bar(range(len(top_protocols)), list(top_protocols.values()))
        ax2.set_xticks(range(len(top_protocols)))
        ax2.set_xticklabels(list(top_protocols.keys()), rotation=45)
        ax2.set_title('å¯èƒ½çš„åè®®åˆ†å¸ƒ')
    
    # 3. å¡«å……é•¿åº¦åˆ†å¸ƒ
    padding_lengths = []
    for packet_data in analysis_data['detailed_analysis']:
        for layer_data in packet_data['layers'].values():
            if 'decoded_data' in layer_data and layer_data['decoded_data']:
                padding_info = layer_data['decoded_data'].get('padding_analysis', {})
                if padding_info.get('padding_detected'):
                    padding_lengths.append(padding_info.get('padding_bytes', 0))
    
    if padding_lengths:
        ax3.hist(padding_lengths, bins=range(1, max(padding_lengths)+2), alpha=0.7)
        ax3.set_title('å¡«å……é•¿åº¦åˆ†å¸ƒ')
        ax3.set_xlabel('å¡«å……å­—èŠ‚æ•°')
        ax3.set_ylabel('é¢‘æ¬¡')
    
    # 4. ç†µå€¼åˆ†æ
    payload_entropies = []
    padding_entropies = []
    
    for packet_data in analysis_data['detailed_analysis']:
        for layer_data in packet_data['layers'].values():
            if 'decoded_data' in layer_data and layer_data['decoded_data']:
                padding_info = layer_data['decoded_data'].get('padding_analysis', {})
                entropy_analysis = padding_info.get('entropy_analysis', {})
                if entropy_analysis:
                    payload_entropies.append(entropy_analysis.get('payload_entropy', 0))
                    padding_entropies.append(entropy_analysis.get('padding_entropy', 0))
    
    if payload_entropies and padding_entropies:
        ax4.scatter(payload_entropies, padding_entropies, alpha=0.6)
        ax4.set_xlabel('è½½è·ç†µå€¼')
        ax4.set_ylabel('å¡«å……ç†µå€¼')
        ax4.set_title('è½½è·vså¡«å……ç†µå€¼å…³ç³»')
        ax4.plot([0, 8], [0, 8], 'r--', alpha=0.5)  # å¯¹è§’çº¿å‚è€ƒ
    
    plt.tight_layout()
    return fig